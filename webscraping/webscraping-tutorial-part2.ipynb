{
 "metadata": {
  "name": "",
  "signature": "sha256:5e30dcd3ae8b14d765d194cf95d977c3bfb837202a6be7c13703fcf015766aed"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1> Scrape Data from Multiple Pages</h1>\n",
      "\n",
      "<p>Sometimes the data we want to extract is scattered across multiple pages. For instance, take a look at <a href=\"http://www.imdb.com/search/title?genres=comedy&sort=boxoffice_gross_us\"> IMDB list</a> of comedy movies sorted by their US box office gross income. If we want to extract information about more than 50 movies, we need to look into more than one page. Assume for instance we are interested in top 250 titles.\n",
      "\n",
      "<p> Looking into different pages of the list, we can see the URLs look like following:\n",
      "<ul>\n",
      "<li> http://www.imdb.com/search/title?genres=comedy&sort=boxoffice_gross_us </li>\n",
      "<li> http://www.imdb.com/search/title?genres=comedy&sort=boxoffice_gross_us&start=51 </li>\n",
      "<li> http://www.imdb.com/search/title?genres=comedy&sort=boxoffice_gross_us&start=101 </li>\n",
      "</ul>\n",
      "</p>\n",
      "<p>Let's define the common part of the url as <em>url_base</em>.</p>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url_base = \"http://www.imdb.com/search/title?genres=comedy&sort=boxoffice_gross_us\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Now let's iterate over different offsets and build the URL for first 5 pages:</p>\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(1,250,50):\n",
      "    # Concatenate two parts of the URL string\n",
      "    url = url_base + \"&start=\" + str(i)\n",
      "    print url"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>As easy as that! We have addresses that contain data we are interested in. We can get links to pages of top 250 movies using following code:</p>\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import time\n",
      "\n",
      "for i in range(1,250,50):\n",
      "    \n",
      "    # Concatenate two parts of the URL string\n",
      "    url = url_base + \"&start=\" + str(i)\n",
      "    \n",
      "    # Get the web page\n",
      "    response = requests.get(url) \n",
      "    soup = BeautifulSoup(response.content)\n",
      "    \n",
      "    # Extract the addresses of movies in the list\n",
      "    for movie in soup.find_all('td', class_ = 'title'):\n",
      "        for link in movie.find_all('a', limit = 1):\n",
      "            print link.text + ': ' + 'http://www.imdb.com' + link.get('href')\n",
      "            # Optional: Get the cast of each movie\n",
      "            \n",
      "    # Limit the rate\n",
      "    time.sleep(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p> Often, addresses of pages we want to load hold a certain structure. Here, we could access pages by changing an offset parameter (start=<em>number</em>). For some other cases, you might see a page parameter changing in the URL (page=<em>number</em>). Investigating the URLs can give you some clues on how you need to manipulate the address and get to pages you are interested in."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Scrape Pages with Dynamic Content</h1>\n",
      "\n",
      "<p>Now, let's target a different website.</p>\n",
      "\n",
      "<p>For this part, we are going to extract comments from <a href=\"http://www.theguardian.com/technology/2015/jan/28/artificial-intelligence-will-not-end-human-race\">a news article</a> on The Guardian. We will use techniques we have learned so far and try to extract the first comment on this article.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://www.theguardian.com/technology/2015/jan/28/artificial-intelligence-will-not-end-human-race'\n",
      "response = requests.get(url)\n",
      "soup = BeautifulSoup(response.content)\n",
      "mydivs = soup.findAll(\"div\", { \"class\" : \"d-comment__body\" }, limit = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Inspecting the comments using developer mode of the browser, we can see the content of a comment is inside a div with class = \"d-comment__body\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mydivs = soup.findAll(\"div\", { \"class\" : \"d-comment__body\" }, limit = 1)\n",
      "\n",
      "for div in mydivs:\n",
      "    print \"Found a comment!\"\n",
      "    print \"Content of First Comment:\"\n",
      "    print div.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Hmmm, nothing is printed! Let's try to check if there is anything found by the findAll function at the first place.</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if (len(mydivs) == 0):\n",
      "    print \"Nothing found by soup.findAll\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>Well, looks like nothing is found. You can check the Page Source in the browser, search for a part of first comment in the HTML code, and see in fact the comment is not there in the source code.</p>\n",
      "\n",
      "<p>By Inspecting the comment section using developer mode on the browser, we can see comments are contained in a div with class = \"discussion__main-comments js-discussion-main-comments\". Yet in the page source code on the browser, this div is empty. The page source on the browser shows the original code for the page, and when the page gets loaded, the comment portion of the website is empty. However, a script at the server side gets run and populates the comment section later.</p>\n",
      "\n",
      "<p>Many websites use the same method to load some portion of their pages. If we load the website using requests, it load the original HTML code, which does not containt the information we are looking for. To scrape these sites, we need to use another approach.</p>\n",
      "\n",
      "<h2>Using Selenium</h2>\n",
      "\n",
      "Selenium is a python library that helps you automate working with the browser. Using Selenium, you can open up a browser window and fully control it with commands in python. This way, you can wait for the browser to fully load data from its scripts, and then use the final code to extract useful information.\n",
      "\n",
      "Try following code to open a Firefox window using Selenium:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from selenium import webdriver\n",
      "\n",
      "browser = webdriver.Firefox()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now fully drive the browser using python. Try following code to load a website on the browser:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://www.imdb.com'\n",
      "browser.get(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p>If we are interested in information that is not there in the original source code and gets loaded later from scripts (like comments on the article from The Guardian), we need to wait for scripts to finish running. Selenium supports waiting for a certain element to show up on the page, but to keep this tutorial short, we are not going to discuss the syntax. Instead, we will just wait for a certain amount of time (5 seconds in the code). Be aware that this time might not be enough in some use cases.</p>\n",
      "\n",
      "<p>Now, let's load our news article, wait for comments to load, and then extract first comment from the final HTML code:</p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://www.theguardian.com/technology/2015/jan/28/artificial-intelligence-will-not-end-human-race'\n",
      "browser.get(url)\n",
      "time.sleep(5)\n",
      "page_content = browser.page_source\n",
      "\n",
      "soup = BeautifulSoup(page_content)\n",
      "mydivs = soup.findAll(\"div\", { \"class\" : \"d-comment__body\" }, limit = 1)\n",
      "\n",
      "for div in mydivs:\n",
      "    print \"Found a comment!\"\n",
      "    print \"Content of First Comment:\"\n",
      "    print div.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the end, we close the browser window:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "browser.quit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h1>Exercise: Find the Nearest Store</h1>\n",
      "\n",
      "<p>Open the <a href=\"http://locations.coffeebean.com/#/map\">store locator</a> for The Coffee Bean. This page locates your address and lists nearest stores to your current location. Write a python script to extract first address in the list. </p>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}